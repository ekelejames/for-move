services:
  kafka:
    image: confluentinc/cp-server:7.6.0
    container_name: kafka
    ports:
      - "29092:29092"
      - "9092:9092"
    environment:
      CLUSTER_ID: 'xVwOh_nFSfq2tXYyxPXDTQ'
      KAFKA_BROKER_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@0.0.0.0:29093'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,INTERNAL:SASL_PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: 'INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:29092,CONTROLLER://0.0.0.0:29093'
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://kafka:9092,EXTERNAL://kafka:29092' # i changed this to kafka from localhost
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8085
      KAFKA_LOG_DIRS: '/var/lib/kafka/data'
      KAFKA_AUTHORIZER_CLASS_NAME: io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
      KAFKA_CONFLUENT_AUDIT_LOG_ENABLED: true
      KAFKA_CONFLUENT_AUDIT_LOG_INCLUDE: kafka.AlterClientQuotas,kafka.AlterConfigs,kafka.AlterIsr,kafka.AlterMirrors,kafka.AlterPartitionReassignments,kafka.AlterReplicaLogDirs,kafka.AlterUserScramCredentials,kafka.CreateAcls,kafka.CreateClusterLinks,kafka.CreatePartitions,kafka.CreateTopics,kafka.DeleteAcls,kafka.DeleteClusterLinks,kafka.DeleteGroups,kafka.DeleteRecords,kafka.DeleteTopics,kafka.ElectLeaders,kafka.IncrementalAlterConfigs,kafka.InitiateShutdown,kafka.OffsetDelete,kafka.RemoveBrokers,kafka.UpdateFeatures

#      KAFKA_CONFLUENT_SECURITY_EVENT_ROUTER_CONFIG: "/config/security-event-router-config.json"

      # enabling acls
      #KAFKA_AUTHORIZER_CLASS_NAME: org.apache.kafka.metadata.authorizer.StandardAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "false"
      KAFKA_SUPER_USERS: "User:admin;User:ANONYMOUS"

      # Specify the sasl mechanisms enabled
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN #SCRAM-SHA-512

      # specifying inter broker protocol
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN #SCRAM-SHA-512

      # Security configuration for external listener
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/jaas.conf"

      # connecting to external listener
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:9092
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'true'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
      CONFLUENT_METRICS_REPORTER_SASL_MECHANISM: PLAIN
      CONFLUENT_METRICS_REPORTER_SECURITY_PROTOCOL: SASL_PLAINTEXT
      CONFLUENT_METRICS_REPORTER_SASL_JAAS_CONFIG: 'org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin-secret";'

      # Configuring debugging level
      KAFKA_LOG4J_ROOT_LOGLEVEL: 'INFO'
      KAFKA_LOG4J_LOGGERS: 'kafka=INFO,kafka.controller=INFO,kafka.log.LogCleaner=INFO,state.change.logger=INFO,kafka.producer.async.DefaultEventHandler=INFO'
      KAFKA_SASL_KERBEROS_SERVICE_NAME: kafka
    volumes:
      - ./kafka-data:/var/lib/kafka/data
      - ./jaas.conf:/etc/kafka/jaas.conf


  # kafka-connect:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.kafka-connect
  #   container_name: kafka-connect
  #   depends_on:
  #     - kafka
  #     - schema-registry
  #   ports:
  #     - "8083:8083"
  #   environment:
  #     CONNECT_BOOTSTRAP_SERVERS: 'PLAINTEXT://kafka:29092'
  #     CONNECT_REST_ADVERTISED_HOST_NAME: 'kafka-connect'
  #     CONNECT_GROUP_ID: 'kafka-connect-group'
  #     CONNECT_CONFIG_STORAGE_TOPIC: 'connect-configs'
  #     CONNECT_OFFSET_STORAGE_TOPIC: 'connect-offsets'
  #     CONNECT_STATUS_STORAGE_TOPIC: 'connect-status'
  #     CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_KEY_CONVERTER: 'org.apache.kafka.connect.storage.StringConverter'
  #     CONNECT_VALUE_CONVERTER: 'io.confluent.connect.json.JsonSchemaConverter'
  #     CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8085
  #     CONNECT_REST_PORT: 8083
  #     CONNECT_PLUGIN_PATH: '/usr/share/java,/usr/share/confluent-hub-components,/kafka-plugins'
  #     CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
  #     CONNECT_LOG4J_LOGGERS: 'org.reflections=ERROR'
  #   volumes:
      # - ./plugins/kafka-plugins:/kafka-plugins
      # - ./metrics/sample-flat-schema-metric/config:/sample-flat-schema-metric-config
      # - ./scripts:/scripts
      # - ./metrics/sample-nested-schema-metric/config:/sample-nested-schema-metric-config
      # - ./mongodb-kafka-connect-mongodb-1.14.1:/usr/share/confluent-hub-components
#    entrypoint: ["sh", "/scripts/init-kafka-connect.sh"]
#    healthcheck:
#      test: ["CMD", "curl", "-f", "http://localhost:8083/"]
#      interval: 30s
#      timeout: 10s
#      retries: 5


  kafka-connect:
    build:
      context: .
      dockerfile: Dockerfile.kafka-connect
    container_name: kafka-connect
    depends_on:
      - kafka
      - schema-registry
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'SASL_PLAINTEXT://kafka:9092'
      CONNECT_REST_ADVERTISED_HOST_NAME: 'kafka-connect'
      CONNECT_GROUP_ID: 'kafka-connect-group'
      CONNECT_CONFIG_STORAGE_TOPIC: 'connect-configs'
      CONNECT_OFFSET_STORAGE_TOPIC: 'connect-offsets'
      CONNECT_STATUS_STORAGE_TOPIC: 'connect-status'
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: 'org.apache.kafka.connect.storage.StringConverter'
      CONNECT_VALUE_CONVERTER: 'io.confluent.connect.json.JsonSchemaConverter'
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8085
      CONNECT_REST_PORT: 8083
      CONNECT_PLUGIN_PATH: '/usr/share/java,/usr/share/confluent-hub-components,/kafka-plugins'
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGERS: 'org.reflections=ERROR'

      CONNECT_SECURITY_PROTOCOL: SASL_PLAINTEXT
      CONNECT_SASL_JAAS_CONFIG: 'org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin-secret";'
      CONNECT_SASL_MECHANISM: PLAIN

      CONNECT_PRODUCER_SECURITY_PROTOCOL: SASL_PLAINTEXT
      CONNECT_PRODUCER_SASL_JAAS_CONFIG: 'org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin-secret";'
      CONNECT_PRODUCER_SASL_MECHANISM: PLAIN

      CONNECT_CONSUMER_SECURITY_PROTOCOL: SASL_PLAINTEXT
      CONNECT_CONSUMER_SASL_JAAS_CONFIG: 'org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin-secret";'
      CONNECT_CONSUMER_SASL_MECHANISM: PLAIN

      #CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY: 'ALL'

    volumes:
      - ./plugins/kafka-plugins:/kafka-plugins
      - ./metrics/sample-flat-schema-metric/config:/sample-flat-schema-metric-config
      - ./scripts:/scripts
      - ./metrics/sample-nested-schema-metric/config:/sample-nested-schema-metric-config
      - ./mongodb-kafka-connect-mongodb-1.14.1:/usr/share/confluent-hub-components
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/"]
      interval: 30s
      timeout: 10s
      retries: 5

# connector.client.config.override.policy
# kafka-acls --bootstrap-server localhost:9092 --command-config adminclient-configs.conf \
#  --add --allow-principal User:<Sink Connector Principal> \
#  --consumer --topic logs --group connect-hdfs-logs


  control-center:
    image: confluentinc/cp-enterprise-control-center:7.6.0
    container_name: control-center
    ports:
      - "9021:9021"
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'PLAINTEXT://kafka:29092'
      # CONTROL_CENTER_BOOTSTRAP_SERVERS: 'SASL_PLAINTEXT://kafka:9092'
      # CONTROL_CENTER_SECURITY_PROTOCOL: SASL_PLAINTEXT
      # CONTROL_CENTER_SASL_MECHANISM: PLAIN
      # CONTROL_CENTER_SASL_JAAS_CONFIG: 'org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin-secret";'
      CONTROL_CENTER_CONNECT_CONNECT_CLUSTER: 'kafka-connect:8083'
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8085'
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      CONTROL_CENTER_CONNECT_HEALTHCHECK_ENDPOINT: '/connectors'
      CONTROL_CENTER_LOG4J_ROOT_LOGLEVEL: 'INFO'
    depends_on:
      - kafka
      - schema-registry


  schema-registry:
    image: confluentinc/cp-schema-registry:7.6.0
    container_name: schema-registry
    depends_on:
      - kafka
    ports:
      - "8085:8085"
    environment:
      - SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=SASL_PLAINTEXT://kafka:9092
      - SCHEMA_REGISTRY_KAFKASTORE_SASL_MECHANISM=PLAIN
      - SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL=SASL_PLAINTEXT
      - SCHEMA_REGISTRY_KAFKASTORE_SASL_JAAS_CONFIG=org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin-secret";
      - SCHEMA_REGISTRY_HOST_NAME=localhost
      - SCHEMA_REGISTRY_LISTENERS=http://0.0.0.0:8085
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:8085/subjects"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
      
  mongo:
    image: mongo
    container_name: mongo
    ports:
      - "27017:27017"
    environment:
      - MONGO_INITDB_ROOT_USERNAME=root
      - MONGO_INITDB_ROOT_PASSWORD=root
    volumes:
      - ./mongo-data:/data/db

  mongo-express:
    image: mongo-express
    container_name: mongo-express
    ports:
      - "8084:8081" # Mapping host port 8084 to container port 8081
    environment:
      - ME_CONFIG_MONGODB_ADMINUSERNAME=root
      - ME_CONFIG_MONGODB_ADMINPASSWORD=root
      - ME_CONFIG_MONGODB_SERVER=mongo
      - ME_CONFIG_BASICAUTH_USERNAME=admin
      - ME_CONFIG_BASICAUTH_PASSWORD=admin_pass
    depends_on:
      - mongo

  mongo-shell:
    image: mongo
    container_name: mongo-shell
    command: ["sleep", "infinity"]
    depends_on:
      - mongo

  # namenode:
  #   image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
  #   container_name: namenode
  #   restart: always
  #   ports:
  #     - 9870:9870
  #     - 9000:9000
  #   volumes:
  #     - hadoop_namenode:/hadoop/dfs/name
  #   environment:
  #     - CLUSTER_NAME=test
  #   env_file:
  #     - ./src/streambed/config/hadoop/hadoop.env

  # datanode:
  #   image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
  #   container_name: datanode
  #   restart: always
  #   volumes:
  #     - hadoop_datanode:/hadoop/dfs/data
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9870"
  #   env_file:
  #     - ./src/streambed/config/hadoop/hadoop.env
  
  # resourcemanager:
  #   image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
  #   container_name: resourcemanager
  #   restart: always
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
  #   env_file:
  #     - ./src/streambed/config/hadoop/hadoop.env

  # nodemanager1:
  #   image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
  #   container_name: nodemanager
  #   restart: always
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
  #   env_file:
  #     - ./src/streambed/config/hadoop/hadoop.env
  
  # historyserver:
  #   image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
  #   container_name: historyserver
  #   restart: always
  #   environment:
  #     SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
  #   volumes:
  #     - hadoop_historyserver:/hadoop/yarn/timeline
  #   env_file:
  #     - ./src/streambed/config/hadoop/hadoop.env

volumes:
  kafka-data:
  connect-plugins:
  mongo-data:
